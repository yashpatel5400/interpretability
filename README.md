# Introduction
Many of the topics we've discussed thus far have dealt with situations in which the machine learning algorithms are black-boxes we wish to investigate. Specifically, the major topics have been discussing fairness in the context of machine learning algorithms and how some definitions thereof are incompatible; how algorithms can use public variables as proxies for sensitive information and pertinent ethics; and how biases are often baked directly into the datasets. In all of these contexts, however, we are attempting to uncover an underlying bias present by using the algorithm outputs. That is to say, all of our discussion up until now have used what the algorithm outputs, rather than attempting to understand how or why such decisions were produced.

Interpretability is the study of just this, although the specifics of its definition differ on context. In fact, the field of interpretability applied in ML is quite nascent. The field has only recently started to blossom, having had only ~4000 papers in 1998 and reaching upwards of 20000 papers in 2018. However, as with most newly born fields, even the basic infrastructure underlying the study of interpretability has yet to be definitively laid. In fact, the definition of "interpretability" has only formally appeared quite recently, in one of the landmark papers of the field: The Mythos of Model Interpretability. This definition, however, is more involved than it may seem at first glance, and so we discuss it after laying out some basic context behind the field. 

# Rationale
Before diving in, studying interpretability of ML algorithms begs the question: why? Even the rationale behind the study of interpretability in machine learning may seem a bit opaque at first. After all, why is it that we need to understand the internals in algorithms at all? Shouldn't looking at example outputs suffice in diagnosing whether there are any biases baked into the algorithm? In fact, one example perfectly captures its significance: "In the study, the goal was to predict the probability of death (POD) for patients with pneumonia so that high-risk patients could be admitted to the hospital while low-risk patients were treated as outpatients" [1]. For this particular case, the investigators were looking into the pros and cons of using complex, multi-layer neural nets vs. simple, more understable rule-based systems. Although they found the NN implementations to have better accuracy when tested on the datasets, they decided to stick with the simpler version, since it helped uncover some alarming internals of the algorithm.

Specifically, it turned out that the the system developed a prediction rule saying if the patient has asthma, he should be categorized as of lower risk of death. This, however, seemed naturally unintuitive. On further investigation, it came to their attention, that the reason this happened was because "patients with a history of asthma who presented with pneumonia usually were admitted not only to the hospital but directly to the ICU" [1]. Had the more complex systems been instituted instead, this behavior would have likely never been discovered in the first place, seeing that tracing the path from input to output is an often hopelessly complicated exercise. Thus, it turns out that interpretability is a keystone property necessary in uncovering and understanding incorrect behaviors of an algorithm. 

Algorithmic errors, such as the one discussed above, largely arise for two reasons: (1) ML systems are intended to be used in prediction problems, but the problem is inherently a causal problem or (2) acting on the ML prediction inherently changes the validity of the original prediction. Specifically, to expound on the latter point, if an algorithm were trained on a particular context, changing that context will often make its predictions either invalid or at least less accurate. For example, the herd mentality in acting on traffic predictions often invalidates those very predictions. In other words, when a prediction is made of a road being traffic-free, people often swarm on those paths, in turn increasing traffic. 

Another case that can seemingly be posed against studying interpretability is that, if our ML algorithms can be made to pass fairness definitions we have in place (through studies from past chapters), does that not mean we can ignore the internals of the algorithm? While this could partially be true, the main issue that arises is that such definitions are quite rigid. After all, it could be the case that the priorities of fairness shift over time. In addition, we've seen in the past that while algorithms may pass certain fairness definitions, we would still not colloquially describe them as "fair," i.e. the COMPAS algorithm. In such cases, where it is unclear how to evaluate whether or not an algorithm is "fair," interpretability is a more appropriate means to the end.

# Defining Interpretability
Having established the importance of studying ML interpretability, we shift focuses to looking at its established definition. In particular, in a simplified framework, interpretability manifest into two main branches: understanding the predictions of an algorithm for a specific example (local understanding) vs. understanding the overall behavior of the algorithm (global understanding). While there is often an interplay between the two, each of these has very different approaches. The local understanding approach can be framed as "interpretable decisions" and the global understanding as "interpretable models," as they respectively seek to interpret a single example decision and the overall model. These two branches can be further separated into seven main sub-branches of interpretability, as posed by The Mythos of Model Interpretability: 
- Interpretable Models
	- Simulatability
	- Algorithm transparency
	- Semantic explanation
- Interpretable Decisions
	- Visualization
	- Local explanation
	- Explanation by example
	- Decomposability

## Simulatability
Simulatability, as the name suggests, formalizes the intuitive idea that a model with two separable variables will be significantly more interpretable as compared to one with several layers of branching predictions. In more precise terms, if an algorithm has a small set of parameters with little interdependence that can all be fit in your head, it can be deemed as "simulatable." In its original form, simulatability captures the idea that "a human should be able to take the input data together with the parameters of the model and in reasonable time step through every calculation required to produce a prediction" [4]. An example of where simulatability comes into play is sparse models, upon which we expand later. Sparse models make it an explicit goal to ensure only a minimal set of the input features are deemed relevant and have non-zero influence on the output. Note that, as with most other definitions, this one is slightly subjective, in that what can be "fit in your head" is highly dependent on the person in question.  

## Decomposability/Intelligibility
In line with keeping a reasonable parameter space, another natural route of defining an "understandable algorithm" is by ensuring all its constituent parts are understandable. In other words, we wish to formalize the notion that understanding the parts and how they interact helps develop an understanding of the final algorithm. Formally, this notion is denoted as "decomposability," in which "each part of the model - each input, parameter, and calculation - admits an intuitive explanation" [4]. In other words, each parameter can be pinned to some real world parallel, rather than solely being a mathematical feature construct.
It is, therefore, quite natural to inquire about examples that are simulatable but not decomposable (or vice versa) seeing that the two are seemingly closely related to the parameter space of the model. While this is true, consider the example of a sparse linear regression, even one that only has a single feature, i.e. Y = W1X + B. This is clearly simulatable by it only have a single relevant input feature. However, this feature could be extremely complicated. For example, if we're trying to predict the gender of a person given an image, this X could be a random weighting of the input pixels that has no real-world interpretation. Thus, having this as a separate facet of the definition is necessary.

## Algorithm transparency
Another concept inherent with interpretability is understanding the process of how the final algorithm came to be. With any supervised ML algorithm, the steps in developing the final, deployed form are (1) setting up the algorithm infrastructure, (2) training it on some fixed training dataset, and (3) testing on a test/validation set. Typically, the algorithms that we encounter are those that have already completed all three of these steps. It is, therefore, often unclear how the training took place, i.e. details of how the error surface is shaped or proving that training will converge to a unique solution. Being able to understand the internals of this process falls into algorithm transparency. 

## Semantic explanation
In addition to having these properties of being able to understand the algorithm itself, interpretability inherently makes it a point to be able to discuss these algorithms. The most natural ways of discussing these algorithms are text-based and image-based. Text-based explanations, naturally referred to as "semantic explanation," would detail in words why a particular decision was made. For example, the algorithm may be trained to classify images on the associate sentiment (i.e. either "happy" or "sad"). If it were accompanied by a textual explanation, the algorithm may output a classification "sad" and its associated explanation "there is a crying baby in the scene." 

The natural follow-up question is: why text? Why not use another medium of communication, such as images? While images and visualizations are also critical in communication (as we discuss further down), text is often the most natural way of explaining these algorithms due to the lack of ambiguity in language. In particular, words are often mapped to a single concept, whereas images often have significantly more routes of interpretations that make them an ineffective manner of getting on the same page. In addition, text often has a more structured format, making it more automatable, should we wish for an algorithm to produce a textual explanation alongside its decision. In other words, since language often has a more rigid structure than images, abiding by those laws to produce a sensical interpretation is easier than similarly doing so in image form.

## Visualization
Having said this, there are certain contexts in which visualization is the more appropriate method of communicating. For the field of computer vision, it seems highly appropriate to make use of visualizations to convey the reasoning behind why a particular decision was made. We look at specific examples of such visualization systems that have been developed later in this chapter, but as a concrete idea to hold on to, an example of a visual explanation may highlight a particular segment of the input image, showing that that was the region that was most critical to determining its eventual classification.

## Local explanation
Much in the way decomposability looks at understanding the whole as a result of understanding the individual parts and their interactions, a means of understanding a particular decision is by understanding the significance of its different features. Much in the way a standard sensitivity analysis might work, by tweaking different parts of the input and seeing how the output changes as a result, we can hold to gain an understanding of how the model is making its classifications. For example, if we are looking at an image classifier that distinguishes between cats and dogs, we could try tweaking a dog input by adjusting its ears to be more pointed and see whether the classification changes. This would, therefore, elucidate the algorithm's sensitivity of the ear shape, and make us aware of whether or not it is a feature used in the classification.

## Explanation by example
Unlike most of the other "interpretable decision" definitions, this one directly makes use of the training set. In particular, in doing a classification of a new example, the model would output which examples from the dataset were "most similar" (by the algorithm's learned internal metric) to that example. This is similar to how k-nearest neighbors works, where the neighbors are determined after mapping the examples to the feature space learned by the algorithm. In turn, we could determine which features were deemed important by the algorithm. For example, if the k-nearest neighbors of an input image of a dog all had oblong shaped objects with a leash attached, we could assume that the shape and presence of a leash were deemed critical in classification.

## Branches of Interpretability Summary
Returning to the dichotomy previous presented between interpretable models vs. decisions, we have yet to discuss why one may be preferred over the other. This, however, draws an awfully close semblance to the issue of group vs. individual fairness respectively, where the former is the aggregate fairness and the latter individual. In line with that, the pertinence of each should be quite clear: the former is much more of interest to the algorithm developer whereas the latter is for the user. Creator may have legal requirements to make the model interpretable, while users typically only care about the reason a particular decision was made. A summary of the dichotomy is presented below.

|           | Interpretable Models              | Interpretable Decisions  |
| --------- |:---------------------------------:| ------------------------:|
| Desired by| Model creator                     | Model users              |
| Type      | - Simulatability                  | Semantic Explanation     |
|           | - Decomposability/Intelligibility | Visualization            |
|           | - Algorithm Transparency          | Local Explanation        |
|           |                                   | Explanation by example   |
| Method of achieving | Built into model        | Post-hoc                 |

# Data Modelling and Interpretability
It may seem from the discussions above that the field of interpretability has emerged solely for modern machine learning applications, where developers and analysts are not hand-baking algorithms but rather applying a set toolbox of black boxes. While this may be true, with the advent of neural networks and their complexity truly spurring an interest in interpretability, it doesn't entirely ring true that the results discussed are not applicable to more traditional data modelling techniques. In fact, we take a quick aside to see what facets of the interpretability definition data modelling is especially apt for. 
Seeing that modellers are attempting to capture their intuition in any model they develop, the most obvious definition prioritized is decomposability. After all, in applying their domain knowledge to construct a model, the developers must have some understanding of which features are critical in producing an output, meaning the parameters of the model must directly map to these interpretable features. Such data modellers, somewhat less obviously, often also focus on ensuring their models have the proper convergence performance necessary when deployed to the real world scenario. After all, for a custom made algorithm/model, it is not so obvious whether convergence is guaranteed in the first place. As a result, there is often a requisite in the field to study and prove such properties. In doing so, the model satisfies the algorithmic transparency requirement, for we can directly see how the training takes place and proofs of its particular properties.
So, while the field of machine learning has largely shifted towards discriminatory modelling, it is quite easy to see that data modelling holds the upper hand in some regards of interpretability, from which discriminatory modelling may borrow going forward.

# Models with built-in interpretability
Most the the discussion so far has focused on the need to make machine learning models interpretable. This may seem to imply that no machine learning model is interpretable. However, there are techniques to build machine learning algorithms which satisfy some of the definitions of interpretability. This then is the setting where the model itself is interpretable. We now describe common techniques used to build interpretable models.


## Technique 1: Rule-based models
Decision trees are rule-based models where the final classification or regression outcome for an input is determined based on a hierarchical set of conditional statements. Leo Breiman and his collaborators introduced the concept of random forests,  which allowed an ensemble of decision trees to be learned while controlling for the inevitable overfitting that occurs with a single decision tree. The interpretability of decision trees arises from the fact that the final classification outcome can be found by tracing a path through the tree. In other words, the model is decomposable into a set of smaller decision rules which when put together, yield the outcome. 

The same theme applies to the learning of disjunctive normal forms (DNFs) of logical formulas, as explored in a recent work by Wang et al. [8]. An example of a DNF used by a human used for selecting a car would be something like 'Toyota Yaris because of mileage AND lot of interior room AND styling" OR "Infinity M-Sedan because it's a hotrod AND cheap'. The interpretability of these models in [8] is derived from human studies, which is a common technique used in many papers in the literature. Advertisers on Facebook also use ORs of ANDs to target users by defining a set of different custom audiences (conjunction) and targeting their ad to any one of these custom audiences (disjunction). Since these models are used by humans, learned rule-based models are also interpretable by humans to a certain extent.

### Limitations
The main drawback of rule-based approaches is that the composite rule used to make a decision could be too long and complex. In the case of decision trees, this would imply that the depth or width, or both are too large, which would affect simulatability. For DNFs, each conjunction could be very complex and combined with a large number of disjunctions, may render the model uninterpretable for difficult tasks.

## Technique 2: Case-based decision making 
The idea behind case-based methods to provide interpretability is very familiar to humans, as we often employ logic like "I recommended this item to you because someone similar to you liked it". In the case of Bayesian Case Models [9], the clustering results are explained by providing examples of objects that are representative of the particular cluster the given object was assigned to, known as 'prototypes'. For example, a taco is representative of other food items such as quesadillas and fajitas, and all of these lie in a 'subspace' that is represented by keywords such as salsa, sour cream and avocado. These 'prototypes' and 'subspaces' are identified by a joint inference over the observed samples.

### Limitations
Case-based reasoning assumes that there exists a good representative example that can be used to explain each cluster. However, in many tasks, no such example may exist. Further, the keywords used to identify the subspace may not actually capture the underlying reasons behind the clustering. For example, while not all Mexican food items may contain 'sour cream' and 'avocado', these may be the keywords used to identify the subspace. If a human were asked to provide keywords for Mexican food, they may instead identify words such as 'corn' or 'beans' which would better represent the intended cluster.

## Technique 3: Inducing model sparsity
### Sparse linear regression
The standard linear regression problem involves the solution of a least squares problem:

min (Y - Z)^2,
Where Z = B0 + B1 * X1 + ... + Bn * Xn

Linear regression has a number of issues with regard to both utility and interpretability. Sparsity can help simultaneously address both types of issues. Having a sparse weight vector can help combat overfitting, which leads to better generalization at test time. In terms of interpretability, linear regression has two specific problems that the addition of sparsity can help address:

1) 'Rashomon effect': This is a phenomenon in which multiple equally accurate models can be learned from the same set of observations, each with different weights and even possibly different support over the weights. While none of the models is an accurate reflection of reality, some models weight important features more highly than others and finding a method to choose between these different models learned from the same data is crucial.

2) Over-abundance of features: While only a small number of features may be sufficient to characterize the output, linear regression often results in a large number of features being given non-zero weights. This large number of features used in the model makes it hard to interpret the model, in the specific sense of 'simulatability' by a human, since simultaneously accounting for a large number of interdependent factors is a difficult cognitive task.

Adding explicit sparsity to the model reduces both the number of features present in the final learned model as well as the number of different possible models.

In order to learn a sparse model, the standard least squares problem can be modified by adding a sparsity inducing prior in the form of a regularization term. The direct way to induce sparsity is to add a L0 constraint on the weight vector B, which provides a count of the number of non-zero elements of B. However, this is a non-convex optimization problem which is hard to solve. Thus, the convex relaxation of this problem is used to induce sparsity, which replaces the L0 constraint by a L1 constraint. This is known as LASSO regression. Adding an L2 constraint to generate the convex ridge regression problem does not induce sparsity, as can be seen geometrically in the diagram below. 

![lasso](https://jamesmccammondotcom.files.wordpress.com/2014/04/screen-shot-2014-04-19-at-11-19-00-pm.png?w=700)

An intuitive explanation for why L1 regression leads to sparsity is as follows: if the component of the squared loss corresponding to a particular weight does not die down fast enough, then the L1regularization term will just set it to 0. The L1 regularization tends to choose features that have a higher correlation with Y. If all features are equally correlated, then LASSO regression will not lead to sparsity, since the loss corresponding to all features will decay at the same rate as their weight is increased.

### Sparse LDA
Latent Dirichlet Allocation (LDA) is a generative model used in Natural Language Processing which allows for the representation of documents as a collection of topics, where each topic determines the probabilities of the appearance of words from the overall vocabulary. While the generative model employed by LDA is interpretable in the sense that it is decomposable, the topics uncovered by LDA for a particular document may not be very informative, since the vocabulary may be very large, and a large number of words under each topic may have non-zero probabilities. To counter this, each topic needs to be sparse. Unstructured sparsity can be induced by adding another prior which enforces the condition that many of the word probabilities under each topic must be zero. Each topic is then represented by a small subset of words that explain most of that topic.
Structured sparsity for LDA can be realized by using 'concept-words' for each topic [10], each of which can then generate a small number of related words with higher probabilities. This sort of structured generative model is appropriate for situations where structured vocabularies already exist. For example, document retrieval tasks or diagnostic tasks often use ontologies. 

### Limitations
Some models may not be able to represent what they have learned from the data in a sparse fashion. In such a case, imposing a sparsity constraint on the data may be counterproductive since the accuracy of the model may be adversely affected. The individual factors identified by a sparse model may not suffice to provide interpretability to users of the model, and only combinations of the factors may provide the interpretation that is desired. Also, existing sparsity techniques may result in outputs that are not sparse enough for interpretability, in that they may contain a large number of features nonetheless.
Limitations of models with built-in interpretability

The models described in this section are typically used for tasks such as estimating disease risk, where there is a high degree of uncertainty to begin with, and increasing model complexity does not lead to significant gains in predictive accuracy. In particular, the main reason is that, in these situations, the answer is not immediately obvious and, therefore, we want to apply these methods to determine whether the answers are correct or not in the first place. For example, returning to the disease risk example, if we are trying to diagnose whether or not a patient has diabetes, the answer is oftentimes not immediately obvious. We, therefore, want to apply the aforementioned tools to boost our confidence in its predictions or alter the algorithms such that it aligns with our natural understanding of the issue at hand.

However, for many present day tasks such as image recognition and natural language processing, there is not much uncertainty about the output, while the automated decision making task is quite difficult and benefits greatly from increased model complexity. For example, given an image of a cat, there is little uncertainty about what the image contains, but until the recent advent of deep neural networks, this was a difficult task for machine learning.  In these cases, building in interpretability could have an adverse impact on the predictive accuracy of the task. This leads us to post-hoc interpretability, where the interpretation of the outputs of complex and accurate models such as deep neural networks is performed after the decision making process is complete, in an attempt to explain the decisions made by the model. 

## Interpretation of Linear Regression (Example)
We now turn to an example of how a seemingly trivial example of interpretation is significantly more nuanced than it may appear at first glance. In particular, we consider the most straightforward example that has been oftentimes qualitatively thought of as the "pinnacle" of understable models: linear regressions. Why is that people believe that such models can be understood intuitively? The reason likely lies in both a combination of how familiar most scientists and engineers are with the technique in addition to the direct ability to claim, "Holding all other variables fixed, we can gather that increasing X has a direct or inverse correlation with Y and has sensitivity Z." 

Turning to a specific example, consider:
Y = B0 + B1 * X1 + ... + Bn * Xn

from before. We specifically consider the latter claim, namely that of being able to directly point to correlations between input features and the output variable. This, in fact, cannot be directly concluded. There is first the issue of having different variances amongst the features. In turn, any sensitivity analysis would be vacuous, given that adjusting some parameter by x cannot be directly extended to another parameter. Of course, this issue has the simple fix of normalization and is, therefore, not that great of an issue. Having said that, it is difficult to say, solely given the model, whether this normalization was performed before it was given to you, making such qualitative inquiries more difficult to make.
The bigger concern at hand is that of correlated input features. Namely, should it be the case that Xa,Xbare correlated, the entire formulation of studying the relation between Xaand Y breaks down, since Xbcannot be fixed in this scenario. One seemingly natural way to get around this issue is through the introduction of "composite features." By this, we mean look at your X1,...,Xnand consider any composite feature that can be created as a combination of those. That is to say, consider (X1),(X1+X2),...,(X1+X2+...+Xn),(X2,),...(Xn), and associate to each a different coefficient, i.e. use these as the input features on which the regression is to be performed. This would capture any possible composite feature, meaning there would never be any correlation between these features, since they each individually are the composites themselves. The main issue is that this is computationally infeasible: the number of composite features is identical to the number of subsets of a set of n elements, which is 2^n.

The more efficient, albeit less intuitive, manner of dealing with this issue was discussed in a paper called Algorithmic Transparency via Quantitative Input Influence. Starting again with the basic formulation, the standard linear regression (under QII) is interpreted as follows: "we measure the difference in the quantity of interest, when the feature i is changed via an intervention" [2]. In other words, they consider the outputs that result from a particular input set, see how that result is affected as a result of replacing one of the features with noise, and in turn determine the critical variables. 

However, this formulation holds one major limitation: it only applies for to independent (non-correlated) features. The extension to handling the interpretation of correlated variables was the main contribution of the paper, the problem statement eloquently put as follows: "In many situations, intervention on a single input variable has no influence on the outcome of a system. Consider, for example, a two-feature setting where features are age (A) and income (I), and the classifier is c (A,I) = (A = old ) && ( I = high ). In other words, the only datapoints that are labeled 1 are those of elderly persons with high income. Now, given a datapoint where A = young, I = low, an intervention on either age or income would result in the same classification" [2]. Clearly, here, age, income are correlated. To solve this problem, we introduce a new notion of Set QII, where we wish to extend the notions of QII beyond individual features, namely the result of intervening on a set of features, paralleling that of the single feature case. 

The direct benefits of introducing such a notion may not be immediately obvious. After all, our objective is to claim an understanding of the input features on the output. To that end, the Set QII exhibits a rather beautiful link to individual (Unary) QII. In particular, consider some feature x. We now look at the marginal effect, which is essentially the change in QII going from consider a set S to the set S U {x}. Naturally, this will vary significantly from set to set (i.e. across the different choices of S), given different correlations the feature x may have. To that end, we consider an aggregate version of this marginal effect and effectively take its average across several choices of S. Of course, as posed above, we cannot efficiently calculate all these subsets, since that would take exponential time. Instead, we must find a heuristic of estimating this value, i.e. choosing an appropriate subset of the set of subsets of S to be representative.

The specific manner of doing this varies from circumstance to circumstance. One way of approaching this problem arises quite naturally in game theory. Specifically, the game theoretic question is referred to as cooperative game theory, aptly named since the cornerstone problem is choosing an optimal sports team. Namely, suppose we have a roster of players, each with their own individual stats and additionally with cooperative stats, i.e. player A is the best individually, but players B and C combine to perform better on average than A if simultaneously on the field. The problem arises in attempting to determine the best combination of players to put on the field. In doing so, a standard approach is determining the marginal impact of adding some player x onto the team and, aggregating over all the subsets of the players, picking the top players in that regard. In a near identical manner, we are finding the impact of a feature x across all the subsets. The theory behind this problem is quite extensively studied in game theory, and the specifics are not of direct relevance here (most procedures follow up with expectations and probabilities). The large takeaway is how, even in the most elementary of models (linear regressions), interpretability is much more nuanced than it may initially seem.

## Interpreting non-linear models
With highly non-linear models such as convolutional neural networks, it is often difficult even to predict how the model outcome will change if a slight perturbation is applied to the input while this is a trivial task in the case of linear models. This leads to one of the key ideas underlying many techniques which aim to interpret highly complex models: compute the influence in the neighbourhood of a data point. Around a given point, with small enough perturbations, the model behaves linearly, so we can attempt to provide local explanations for model behaviour. Now, we look at a number of techniques which have been used to interpret the decisions made by complex models.

### Local Interpretable Model-agnostic Explanations (LIME)
Starting with the assumption that there do exist some models that are better suited to interpretability by humans, Riberio et al. [11] set up the problem of providing local explanations for a model as an optimization problem, where the model complexity and a loss function pertaining to the inaccuracy of the predictions made by the simple local models in the vicinity of a given data point are simultaneously minimized. In other words, they provide a formulation by which the best possible local explanation in the chosen family of simple linear models. As a concrete choice of simple models, they use sparse linear models to attempt to explain the decisions made by Google's Inception network. 

A major advantage of this formulation is that only black-box access is needed to the model for which explanations are being generated, since the optimization problem depends only on the target model's outputs. However, both accuracy and speed are a concern for this method. The quality of the explanations depends on how accurately the chosen model can replicate the decisions of the target model. Further, a non-trivial optimization problem requiring numerous forward passes to obtain prediction outcomes from the target model has to be solved to generate an explanation for a single example. To explain a single decision made by the Inception network, their framework takes ten minutes. Another drawback is that the representation of the inputs used for the simple models may not lead to an acceptable explanation for the decision outcome. For example, if a neural network classifies sepia-toned images as 'retro', using sparse linear models acting on groups of pixels cannot explain the outcome, since the explanation in this case likely relies on providing a representative example.

### Saliency maps 
Saliency maps [12] are used to find the influence of each input dimension on the classification outcome. In the case of images, this allows for the creation of an influence map which can be visualized. To calculate each pixel's influence on the classification outcome, the image is first classified by the network for which interpretability is desired. This requires a single forward pass through the network. Once the output has been obtained, the gradient of the score for the most likely class is taken with respect to the input and its magnitude is visualized. This answers the question 'Which pixel should be changed to affect the classification output the most?' and thus indirectly indicates which pixels were considered the most important by the network for classification.

### Maximally activating neurons
One of the main issues with deep neural networks is that due to the large number of layers and the non-linear functions applied at each layer, it is hard to interpret what the representation of an input is in the hidden layers. One method to determine the representation learned by each neuron is to determine what input maximally activates it [13]. Starting from a random, noisy image and by performing gradient descent on the output of the neuron whose representation is to be found, the maximally activating input can be found.
A similar idea can be used in the context of unsupervised learning from images, where the task is to determine if an unsupervised network can learn the concept of faces from unlabeled data [14]. In order to determine if a particular neuron was able to distinguish between faces and the background, a histogram of activations of the entire test set for that neuron was found. If there was a significant difference in output activation values, then the neuron was said to be able to recognize faces. In order to determine if the input that maximized the output was indeed an image of a face, the maximally activating inputs from the test set were determined, and gradient descent was also used to find the maximally activating input, starting from noise.

This idea can also be extended to the 'debugging' of neural networks, where the fact that neurons in the network have learned incorrect concepts can be determined by finding these maximally activating inputs.

### Semantic alignment
Using an image dataset annotated with human-labeled visual concepts, Bau et al. [16] aim to quantify how the neurons in the hidden layer react to the presence of these known concepts in input images. Having collected the response of the hidden variables to the input concepts, they determine how aligned each hidden variable is to a particular concept such as texture or objects like grass in the background.

### Localization-based methods
A slightly different take from saliency maps on understanding how the output changes with respect to parts of the input is taken by localization-based approaches such as Grad-CAM [15]. Acting on the intuition that the final layer feature maps learned by a convolutional neural network are representative of the input image, the gradient of the output score for each class is found with respect to the neurons of each final layer feature map. These gradients are then aggregated for each feature map to obtain neuron importance weights, which are then used to form a weighted average of the final layer maps. This coarse heat-map is then scaled back to the size of the original image to determine which area of the input affected the output the most. 

### Input perturbation methods
In these techniques, the aim is to estimate the influence of a single training point on the final model that is generated and its prediction. The naive method of estimating this influence is to remove the training point from the input data and run the entire training process again. However, this is a very inefficient way of determining the influence of training data on the final model generated. Using the idea of influence functions from robust statistics, Koh and Liang [17] speed up this process considerably. Computing the influence a single training point is equivalent to finding the gradient of the learned weights with respect to the increase in weight that point is given in the training data. This leads to an expression that depends on the inverse of the Hessian of the model. Since even this is very computationally expensive, methods based on conjugate gradients and stochastic estimation are used to quickly estimate the inverse of the Hessian. This method requires oracle access to the gradients and Hessian vector products. 
By using this method to find and rank the influence of each training point on the learned model, the user can determine which training points most influenced the prediction a given input receives. For example, they found that while images of fishes were the most helpful for an SVM to classify other images of fishes, images of dogs were sometimes quite helpful for the Inception neural network to classify fishes. This might indicate that neural networks use differences in images to aid in classification, and not just similarities.

### Identifying spurious correlations
Methods such as LIME and Grad-CAM can also be used to identify spurious correlations between inputs and outputs that lead to incorrect classification. In other words, these methods can used to perform differential analysis when the ground truth labels are known. For example, given an image of a husky classified as a wolf, the sparse linear explanation generated by LIME indicates that the most important pixels used by the neural network classifier were not the pixels corresponding to the face of the animal, but rather those corresponding to the background. This aids interpretability since there is now an explanation of why the misclassification occured. Similarly, Grad-CAM can indicate that the portion of the image that caused the image of a female doctor to be classified as a nurse corresponds to the facial region of the input. This represents a bias that the classifier has picked up from the training data, where most doctors are male and most nurses female. When the classifier is trained on a input dataset where these imbalances are corrected, Grad-CAM shows that the classifier now focuses on the aspects of the input that a human would use to classify, such as the lab coat and the presence of a stethoscope.

# Comparison of built-in and post hoc interpretability
In terms of the flexibility afforded to practitioners of machine learning by these various methods, built-in methods are the least flexible, since they require modifying the training procedure as well as the model creation process in order to provide interpretability. Gradient based methods such as Grad-CAM are more flexible since they work on existing models, but require complete access to the model. The advantage that LIME offers in this regard is that it is a completely black-box method of generating explanations and thus affords the most flexibility.

# Works Cited
[1] Caruana et al, [Intelligible Models for Healthcare](http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf)
[2] Datta et al, [Algorithmic Transparency via Quantitative Input Influence](https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf)
[3] Doshi-Velez and Kim, [Towards a Rigorous Science of Interpretable Machine Learning](https://arxiv.org/pdf/1702.08608.pdf) 
[4] Lipton, [The Mythos of Model Interpretability](https://arxiv.org/pdf/1606.03490.pdf)
[5] Ribeiro et al, ["Why Should I Trust You?" Explaining the Predictions of Any Classifier](http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)
[6] Selvaraju et al., [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)
[7] Breiman et al., [Classification and regression trees](https://books.google.com/books/about/Classification_and_Regression_Trees.html?id=JwQx-WOmSyQC)
[8] Wang et al., [Bayesian Or's of And's for Interpretable Classification with Application to Context Aware Recommender Systems](https://arxiv.org/abs/1504.07614)
[9] Kim et al., [The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification](https://arxiv.org/abs/1503.01161)
[10] Doshi-Velez et al., [Graph-Sparse LDA: A Topic Model with Structured Sparsity](https://arxiv.org/abs/1410.4510)
[11] Ribeiro et al., ["Why Should I Trust You?" Explaining the Predictions of Any Classiﬁer](https://arxiv.org/abs/1602.04938)
[12] Simonyan et al., [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)
[13] Erhan et al., [Understanding Representations Learned in Deep Architectures](http://www.dumitru.ca/files/publications/invariances_techreport.pdf)
[14] Le et al., [Building high level features using large scale unsupervised learning](https://arxiv.org/abs/1112.6209)
[15] Selvaraju et al., [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)
[16] Bau et al., [Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796)
[17] Koh and Liang, [Understanding Black-box Predictions via Inﬂuence Functions](https://arxiv.org/abs/1703.04730)
